{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuncaich/TFG_Proyect/blob/main/TFG/MedMNIST_Segmentacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VeORGo7DDFO"
      },
      "outputs": [],
      "source": [
        "#!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAWNMTf0czbA",
        "outputId": "259ec83f-0d7d-4696-e2e2-a7c8759c5de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC1jQWI18ZJk",
        "outputId": "42349eed-b6aa-4e2f-9fd3-7a0e3a5dc81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.1.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 48 kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.1.1.2\n"
          ]
        }
      ],
      "source": [
        "pip install SimpleITK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FG0dpIkoDDks"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from skimage import io\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "#import medmnist\n",
        "#from medmnist import INFO, Evaluator\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import os\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gYpmV1jtC1SV"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001\n",
        "url_imagen = \"/content/drive/MyDrive/Segmentacion/Imagen_Seccionado_128x128\"\n",
        "url_label = \"/content/drive/MyDrive/Segmentacion/Label_Seccionado_128x128\"\n",
        "url_imagen_test= \"/content/drive/MyDrive/Segmentacion/Imagenes_test_128x128\"\n",
        "url_label_test = \"/content/drive/MyDrive/Segmentacion/Label_test_128x128\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K3F0KCPWDVpq"
      },
      "outputs": [],
      "source": [
        "class ImagesCustomDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir_images, root_dir_label, transform=None):\n",
        "        self.transform = transform\n",
        "        volumes = list(filter(lambda x: True if 'volume-' in x else False, os.listdir(root_dir_images)))\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "        for volume_id in volumes:\n",
        "          img_dir = os.path.join(root_dir_images, volume_id)\n",
        "          target_dir = os.path.join(root_dir_label, volume_id)\n",
        "          samples = list(map(lambda x: os.path.join(img_dir, x), sorted(os.listdir(img_dir))))\n",
        "          labels = list(map(lambda x: os.path.join(target_dir, x), sorted(os.listdir(target_dir))))\n",
        "\n",
        "          self.data.extend(samples)\n",
        "          self.targets.extend(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.data[idx])\n",
        "        target = np.load(self.targets[idx])\n",
        "        if self.transform:\n",
        "          img=self.transform(img)\n",
        "\n",
        "        return (img, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MpR9alnMzXn1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ef0b9218-2daa-4054-fbe4-6b242d321443"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\\ntrain_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=True)\\ntest_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=True)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "'''transform = transforms.Compose([ transforms.Resize((64,64)),\n",
        "transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])'''\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "          ])\n",
        "\n",
        "# load the data\n",
        "\n",
        "dataset = ImagesCustomDataset(url_imagen,url_label, transform=transform)\n",
        "train_dataset,train_dataset_eval = torch.utils.data.random_split(dataset,[11082,5000])\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "\n",
        "dataloaders = {\n",
        "    'train': data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True),\n",
        "    'val': data.DataLoader(dataset=train_dataset_eval, batch_size=2*BATCH_SIZE, shuffle=True)\n",
        "}\n",
        "\n",
        "'''\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=True)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=True)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yGlSj1T10HF"
      },
      "outputs": [],
      "source": [
        "class UNetEncode(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encode = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True)\n",
        "        )\n",
        "\n",
        "        self._result = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self._result = self.encode(x)\n",
        "        return self._result\n",
        "\n",
        "    def getResult(self):\n",
        "        return self._result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSxWYu7q13Ol"
      },
      "outputs": [],
      "source": [
        "class UNetDecode(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, upsample=True, kaiming_initialization=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decode(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIbWMsIp13t1"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, output_channels, input_channels=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        #TODO!!\n",
        "        self.encode_lvl_0 = UNetEncode(input_channels, 64)\n",
        "        self.encode_lvl_1 = UNetEncode(64, 128)\n",
        "        self.encode_lvl_2 = UNetEncode(128, 256)\n",
        "        self.encode_lvl_3 = UNetEncode(256, 512)\n",
        "        self.encode_lvl_4 = UNetEncode(512, 1024)\n",
        "\n",
        "        self.decode_lvl_4 = UNetDecode(1024, 1024) #Bottleneck\n",
        "        self.decode_lvl_3 = UNetDecode(512 + 1024, 512)\n",
        "        self.decode_lvl_2 = UNetDecode(256 + 512, 256)\n",
        "        self.decode_lvl_1 = UNetDecode(128 + 256, 128)\n",
        "        self.decode_lvl_0 = UNetDecode(64 + 128, 64)\n",
        "        \n",
        "        self.output = nn.Conv2d(64, output_channels, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        x = self.encode_lvl_0(x)\n",
        "        x = self.encode_lvl_1(x)\n",
        "        x = self.encode_lvl_2(x)\n",
        "        x = self.encode_lvl_3(x)\n",
        "        x = self.encode_lvl_4(x)\n",
        "\n",
        "        # Decode\n",
        "        x = self.decode_lvl_4(x)\n",
        "        x = torch.cat([x, self.encode_lvl_3.getResult()], dim=1)\n",
        "        x = self.decode_lvl_3(x)\n",
        "        x = torch.cat([x, self.encode_lvl_2.getResult()], dim=1)\n",
        "        x = self.decode_lvl_2(x)\n",
        "        x = torch.cat([x, self.encode_lvl_1.getResult()], dim=1)\n",
        "        x = self.decode_lvl_1(x)\n",
        "        x = torch.cat([x, self.encode_lvl_0.getResult()], dim=1)\n",
        "        x = self.decode_lvl_0(x)\n",
        "        \n",
        "        # Output\n",
        "        return self.output(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq1vTHnN2EeV",
        "outputId": "08c3ee13-fb9f-464a-b468-24186d865161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (encode_lvl_0): UNetEncode(\n",
              "    (encode): Sequential(\n",
              "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "  )\n",
              "  (encode_lvl_1): UNetEncode(\n",
              "    (encode): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "  )\n",
              "  (encode_lvl_2): UNetEncode(\n",
              "    (encode): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "  )\n",
              "  (encode_lvl_3): UNetEncode(\n",
              "    (encode): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "  )\n",
              "  (encode_lvl_4): UNetEncode(\n",
              "    (encode): Sequential(\n",
              "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    )\n",
              "  )\n",
              "  (decode_lvl_4): UNetDecode(\n",
              "    (decode): Sequential(\n",
              "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    )\n",
              "  )\n",
              "  (decode_lvl_3): UNetDecode(\n",
              "    (decode): Sequential(\n",
              "      (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    )\n",
              "  )\n",
              "  (decode_lvl_2): UNetDecode(\n",
              "    (decode): Sequential(\n",
              "      (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    )\n",
              "  )\n",
              "  (decode_lvl_1): UNetDecode(\n",
              "    (decode): Sequential(\n",
              "      (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    )\n",
              "  )\n",
              "  (decode_lvl_0): UNetDecode(\n",
              "    (decode): Sequential(\n",
              "      (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): Upsample(scale_factor=2.0, mode=bilinear)\n",
              "    )\n",
              "  )\n",
              "  (output): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "model = UNet(n_classes, 1).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_pYrI-42smN"
      },
      "source": [
        "\n",
        "<h1>Loss Function</h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAkckX3i23Tt"
      },
      "outputs": [],
      "source": [
        "def dice_loss(pred, target, smooth = 1.):\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()    \n",
        "\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "\n",
        "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
        "    \n",
        "    return loss.mean()\n",
        "\n",
        "def iou_loss(pred, target, smooth = 1e-4):\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()    \n",
        "\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    union = pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2)\n",
        "      \n",
        "    loss = (1 - ((intersection + smooth) / (union + smooth)))\n",
        "    \n",
        "    return loss.mean()\n",
        "\n",
        "    \n",
        "def tversky_index(pred, target, alpha , beta,smooth = 1e-4):\n",
        "    pred = pred.contiguous()\n",
        "    target = target.contiguous()    \n",
        "\n",
        "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "    union = pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2)\n",
        "\n",
        "    X_Y = pred.sum(dim=2).sum(dim=2) - intersection\n",
        "    Y_X = target.sum(dim=2).sum(dim=2) - intersection\n",
        "\n",
        "    loss = (1 - (intersection + smooth)/ ((intersection + smooth) + alpha * X_Y + beta * Y_X))\n",
        "    \n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HuG0l8kS-ji"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calc_loss_dice(pred, target, metrics, bce_weight=0.1):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "        \n",
        "    pred = torch.sigmoid(pred)\n",
        "    dice = dice_loss(pred, target)\n",
        "    \n",
        "    #loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "    loss = bce * bce_weight + dice\n",
        "    \n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def calc_loss_iou(pred, target, metrics, bce_weight=0.1):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "        \n",
        "    pred = torch.sigmoid(pred)\n",
        "    iou = iou_loss(pred, target)\n",
        "    \n",
        "    #loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "    loss = bce * bce_weight + iou\n",
        "    \n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['iou'] += iou.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def calc_loss_tversky(pred, target, metrics, bce_weight=0.1):\n",
        "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
        "        \n",
        "    pred = torch.sigmoid(pred)\n",
        "    # Alpha, beta values (https://arxiv.org/pdf/1810.07842v1.pdf)\n",
        "    alpha=.7\n",
        "    beta=.3\n",
        "    tversky = tversky_index(pred, target, alpha , beta)\n",
        "    \n",
        "    #loss = bce * bce_weight + dice * (1 - bce_weight)\n",
        "    loss = bce * bce_weight + tversky\n",
        "    \n",
        "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
        "    metrics['tversky'] += tversky.data.cpu().numpy() * target.size(0)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "def print_metrics(metrics, epoch_samples, phase):    \n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "        \n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        \n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    print(\"LR\", param_group['lr'])\n",
        "                    \n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "            \n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)        \n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = calc_loss_dice(outputs, labels, metrics)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0)\n",
        "\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                print(\"saving best model\")\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eY5Vqc58lKN",
        "outputId": "f29f5927-e493-455d-96e0-87db2e8a836f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/88 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
            "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n",
            "100%|██████████| 88/88 [52:27<00:00, 35.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: bce: 0.276263, dice: 0.552535, loss: 0.414399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [23:12<00:00, 69.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val: bce: 0.112415, dice: 0.278635, loss: 0.195525\n",
            "saving best model\n",
            "75m 40s\n",
            "Epoch 1/4\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [00:48<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: bce: 0.115957, dice: 0.255739, loss: 0.185848\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:20<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val: bce: 0.109165, dice: 0.248412, loss: 0.178788\n",
            "saving best model\n",
            "1m 10s\n",
            "Epoch 2/4\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [00:49<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: bce: 0.081573, dice: 0.254107, loss: 0.167840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:20<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val: bce: 0.036322, dice: 0.233751, loss: 0.135037\n",
            "saving best model\n",
            "1m 10s\n",
            "Epoch 3/4\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [00:48<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: bce: 0.065365, dice: 0.242124, loss: 0.153745\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:20<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val: bce: 0.035014, dice: 0.209137, loss: 0.122076\n",
            "saving best model\n",
            "1m 9s\n",
            "Epoch 4/4\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 88/88 [00:48<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: bce: 0.024724, dice: 0.150826, loss: 0.087775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:20<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val: bce: 0.019789, dice: 0.134768, loss: 0.077278\n",
            "saving best model\n",
            "1m 10s\n",
            "Best val loss: 0.077278\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
        "\n",
        "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=5)\n",
        "#torch.save(model, \"/content/drive/MyDrive/Segmentacion/lstmmodelgpu.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImagesCustomDataset(url_imagen_test,url_label_test,transform=transform)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "a3onEvPQrRFS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LdozH-I0M325"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_datasets(test_loader,model):\n",
        "  batch_predictions = []\n",
        "  model.eval()\n",
        "  dice_loss_list = []\n",
        "  tversky_loss = []\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for idx, (inputs,labels) in enumerate(test_loader):\n",
        "      print(idx)\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      pred = model(inputs)\n",
        "      pred = torch.sigmoid(pred)\n",
        "      tversky_loss.append(tversky_index(pred,labels,0.3 , 0.4).cpu())\n",
        "      dice_loss_list.append(dice_loss(pred, labels).cpu())\n",
        "      #accuracy.append(accuracy_score(labels.cpu(),pred.cpu()))\n",
        "      batch_predictions.append(pred.cpu())\n",
        "      \n",
        "    return np.array(dice_loss_list).mean() , np.array(tversky_loss).mean()        \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = torch.load('/content/drive/MyDrive/Segmentacion/lstmmodelgpu.pth')\n",
        "\n",
        "dice,tversky = test_datasets(test_loader,model)"
      ],
      "metadata": {
        "id": "iLtwjUrT-_ux"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MedMNIST Segmentacion.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}